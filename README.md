# ğŸ” Busca Fornecedor

Sistema de construÃ§Ã£o automÃ¡tica de perfis de empresas B2B brasileiras.

## ğŸ“– DocumentaÃ§Ã£o

**[Acesse a documentaÃ§Ã£o completa](docs/index.html)** - VisualizaÃ§Ã£o interativa do fluxo do sistema, parÃ¢metros, mÃ©tricas e dashboard de monitoramento.

> Estado atual: repositÃ³rio enxuto com apenas a API principal (FastAPI) e os artefatos do dashboard. Testes, scripts de diagnÃ³stico e logs foram removidos.

## ğŸ¯ Objetivo

Construir perfis completos de empresas em atÃ© **90 segundos** com taxa de sucesso de **~80%**.

## ğŸ“Š MÃ©tricas (Ãšltimo Stress Test)

| MÃ©trica | Valor |
|---------|-------|
| Throughput | 155 empresas/min |
| Taxa de Sucesso | 79.7% |
| Tempo MÃ©dio | 72s |
| RAM (300 paralelo) | ~3.5GB |

## ğŸ—ï¸ Arquitetura

O sistema Ã© composto por 3 etapas principais:

1. **Discovery** (~8s) - Busca do site oficial via Serper API + LLM
2. **Scrape** (~45s) - ExtraÃ§Ã£o de conteÃºdo com curl_cffi e estratÃ©gias adaptativas
3. **Profile** (~12s) - AnÃ¡lise LLM (Gemini/OpenAI) para extraÃ§Ã£o estruturada

## ğŸš€ InÃ­cio RÃ¡pido

### Requisitos

- Python 3.11+
- API Keys: Serper, Gemini, OpenAI (opcional), WebShare (opcional)

### InstalaÃ§Ã£o

```bash
# Clone o repositÃ³rio
git clone <repo-url>
cd busca_fornecedo_crawl

# Crie o ambiente virtual
python -m venv venv
source venv/bin/activate

# Instale dependÃªncias
pip install -r requirements.txt

# Configure variÃ¡veis de ambiente
cp .env.example .env
# Edite .env com suas API keys
```

### Uso

```bash
# Iniciar servidor
uvicorn app.main:app --reload

# Testar endpoint
curl -X POST http://localhost:8000/monta_perfil \
  -H "Content-Type: application/json" \
  -H "X-API-Key: sua-api-key" \
  -d '{
    "razao_social": "EMPRESA LTDA",
    "nome_fantasia": "EMPRESA",
    "cnpj": "12345678000199",
    "municipio": "SÃ£o Paulo",
    "uf": "SP"
  }'
```

## âš™ï¸ ConfiguraÃ§Ã£o

| VariÃ¡vel | DescriÃ§Ã£o | ObrigatÃ³rio |
|----------|-----------|-------------|
| `SERPER_API_KEY` | API key do Serper.dev | âœ… |
| `GEMINI_API_KEY` | API key do Google Gemini | âœ… |
| `OPENAI_API_KEY` | API key da OpenAI | Fallback |
| `WEBSHARE_API_KEY` | API key do WebShare | Opcional |
| `API_KEY` | Chave de autenticaÃ§Ã£o | âœ… |

## ğŸ“ Estrutura

```
busca_fornecedo_crawl/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/                    # Endpoints FastAPI
â”‚   â”œâ”€â”€ core/                   # ConfiguraÃ§Ãµes e utilitÃ¡rios
â”‚   â”‚   â”œâ”€â”€ chunking/           # MÃ³dulo de Chunking v4.0 (NOVO)
â”‚   â”‚   â”‚   â”œâ”€â”€ config.py       # ConfiguraÃ§Ãµes centralizadas
â”‚   â”‚   â”‚   â”œâ”€â”€ preprocessor.py # DeduplicaÃ§Ã£o de linhas
â”‚   â”‚   â”‚   â”œâ”€â”€ chunker.py      # DivisÃ£o em chunks
â”‚   â”‚   â”‚   â””â”€â”€ validator.py    # ValidaÃ§Ã£o de chunks
â”‚   â”‚   â””â”€â”€ token_utils.py      # UtilitÃ¡rios de tokenizaÃ§Ã£o
â”‚   â”œâ”€â”€ schemas/                # Modelos Pydantic
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ agents/             # Agentes LLM
â”‚       â”œâ”€â”€ concurrency_manager/# OrquestraÃ§Ã£o global de recursos (v3.0)
â”‚       â”‚   â”œâ”€â”€ global_orchestrator.py  # Balanceamento entre mÃ³dulos
â”‚       â”‚   â”œâ”€â”€ resource_pool.py        # Pool de recursos
â”‚       â”‚   â””â”€â”€ priority_queue.py       # Fila de prioridades
â”‚       â”œâ”€â”€ discovery/          # Busca de sites (lÃ³gica de negÃ³cio)
â”‚       â”œâ”€â”€ discovery_manager/  # Controle de APIs externas (v3.0)
â”‚       â”‚   â”œâ”€â”€ serper_manager.py       # Rate limiting Serper
â”‚       â”‚   â”œâ”€â”€ search_cache.py         # Cache de buscas
â”‚       â”‚   â””â”€â”€ google_search_manager.py # Fallback
â”‚       â”œâ”€â”€ llm_manager/        # Gerenciamento de chamadas LLM
â”‚       â”œâ”€â”€ profile_builder/    # ConstruÃ§Ã£o de perfis
â”‚       â”œâ”€â”€ scraper/            # ExtraÃ§Ã£o de conteÃºdo (lÃ³gica de negÃ³cio)
â”‚       â””â”€â”€ scraper_manager/    # Controle de infraestrutura (v3.0)
â”‚           â”œâ”€â”€ concurrency_manager.py  # SemÃ¡foros por domÃ­nio
â”‚           â”œâ”€â”€ proxy_manager.py        # Pool de proxies
â”‚           â”œâ”€â”€ circuit_breaker.py      # Circuit breaker centralizado
â”‚           â””â”€â”€ rate_limiter.py         # Rate limiting por domÃ­nio
â”œâ”€â”€ docs/                       # Dashboard e documentaÃ§Ã£o interativa
â””â”€â”€ requirements.txt            # DependÃªncias do projeto
```

## ğŸ”§ PadrÃµes e Tecnologias

- **Framework**: FastAPI
- **HTTP Client**: curl_cffi (sem browser headless)
- **LLM**: Google Gemini (primÃ¡rio), OpenAI (fallback)
- **Busca**: Serper.dev (Google Search API)
- **Proxies**: WebShare (rotating residential)
- **ValidaÃ§Ã£o**: Pydantic v2
- **Scraping**: Batch processing (mini-batches com delays variÃ¡veis)
- **ConcorrÃªncia**: Token Bucket + SemÃ¡foros por domÃ­nio (v3.0)
- **ResiliÃªncia**: Circuit Breaker com estados CLOSED/OPEN/HALF_OPEN (v3.0)
- **Chunking**: MÃ³dulo isolado v4.0 com deduplicaÃ§Ã£o e validaÃ§Ã£o automÃ¡tica (v4.0)

## ğŸ“ DecisÃµes Arquiteturais

1. **Sem Browser Headless**: Por restriÃ§Ã£o de RAM do servidor (Playwright usa ~400MB/instÃ¢ncia)
2. **EstratÃ©gias Adaptativas**: FAST â†’ STANDARD â†’ ROBUST â†’ AGGRESSIVE
3. **Sistema RESCUE**: Tenta subpages quando main page tem < 500 chars
4. **Circuit Breaker**: Evita bater em domÃ­nios problemÃ¡ticos (v3.0: estados CLOSED/OPEN/HALF_OPEN)
5. **Batch Scraping**: Meio termo entre sequencial e paralelo (3-5x mais rÃ¡pido, simula navegaÃ§Ã£o humana)
6. **SeparaÃ§Ã£o NegÃ³cio/Infraestrutura**: Managers centralizados para controle de recursos (v3.0)
7. **OrquestraÃ§Ã£o Global**: VisÃ£o unificada de todos os recursos do sistema (v3.0)
8. **Chunking Isolado**: MÃ³dulo independente com deduplicaÃ§Ã£o, chunking e validaÃ§Ã£o (v4.0: reduz ~94% tokens em casos repetitivos)

## ğŸ“Š Monitoramento

- Logs estruturados com timestamps
- MÃ©tricas de performance por etapa
- Tracking de falhas por domÃ­nio
- RelatÃ³rios JSON detalhados
- Status em tempo real dos managers (v3.0)
- Dashboard em tempo real: http://localhost:8000/monitor

## ğŸ§ª Testes de Performance

O projeto inclui testes focados para cada etapa do pipeline:

### Teste de Discovery

Avalia e otimiza a performance da etapa de discovery (busca de sites oficiais).

```bash
# Teste rÃ¡pido (10 empresas)
python tests/discovery/test_discovery_performance.py --empresas 10

# Teste mÃ©dio (50 empresas)
python tests/discovery/test_discovery_performance.py --empresas 50

# Teste completo (100 empresas)
python tests/discovery/test_discovery_performance.py --empresas 100

# Ou use o script interativo
./tests/discovery/exemplo_uso.sh
```

**MÃ©tricas coletadas:**
- Taxa de sucesso
- Tempo mÃ©dio, mediano, min/max
- Percentis (P50, P75, P90, P95, P99)
- Throughput (empresas/segundo)
- Tipos de falha categorizados

**Resultados salvos em:** `tests/discovery/results/`
- `test_results_[timestamp].json` - Resultados detalhados
- `test_statistics_[timestamp].json` - EstatÃ­sticas agregadas
- `test_log_[timestamp].txt` - Logs completos

**DocumentaÃ§Ã£o completa:** [tests/discovery/README.md](tests/discovery/README.md)

### MÃ³dulo de Chunking v4.0

O mÃ³dulo de chunking Ã© responsÃ¡vel por dividir conteÃºdo grande em chunks menores respeitando limites de tokens para processamento LLM.

```bash
# Testar chunking completo
python tests/test_chunking_module.py
```

**Funcionalidades:**
- **PrÃ©-processamento**: DeduplicaÃ§Ã£o de linhas repetidas (economiza atÃ© 94% de tokens)
- **Chunking**: DivisÃ£o inteligente por pÃ¡ginas, parÃ¡grafos e linhas
- **ValidaÃ§Ã£o**: Garantia de que chunks estÃ£o dentro dos limites
- **PreservaÃ§Ã£o**: 100% do conteÃºdo Ãºnico preservado

**Uso:**
```python
from app.core.chunking import process_content, get_chunking_config

# Pipeline completo: preprocess â†’ chunk â†’ validate
chunks = process_content(raw_content)

# Acessar conteÃºdo de cada chunk
for chunk in chunks:
    print(f"Chunk {chunk.index}/{chunk.total_chunks}: {chunk.tokens} tokens")
    content = chunk.content
```

**ConfiguraÃ§Ã£o:**
- Arquivo: `app/configs/chunking/chunking.json`
- Limite padrÃ£o: 20,000 tokens por chunk
- Effective max: 14,705 tokens (considerando overhead)
- DeduplicaÃ§Ã£o: Ativada por padrÃ£o

**Resultados:**
- Economia mÃ©dia: ~94% de tokens em arquivos repetitivos
- Performance: <20ms por arquivo
- ValidaÃ§Ã£o: 100% dos chunks dentro dos limites

## ğŸ› Erros Comuns

| Erro | Causa | MitigaÃ§Ã£o |
|------|-------|-----------|
| ConteÃºdo Insuficiente | Site SPA ou main page vazia | Sistema RESCUE |
| Site NÃ£o Encontrado | Empresa sem presenÃ§a online | MÃºltiplas queries |
| Timeout | Site lento ou proteÃ§Ã£o | EstratÃ©gias adaptativas |

## ğŸ“œ Changelog

### v4.0 (Atual)
- âœ… MÃ³dulo de Chunking isolado e reestruturado
- âœ… DeduplicaÃ§Ã£o de linhas repetidas (economia de ~94% tokens)
- âœ… ValidaÃ§Ã£o automÃ¡tica de chunks
- âœ… ConfiguraÃ§Ãµes centralizadas em JSON
- âœ… Pipeline completo: preprocess â†’ chunk â†’ validate
- âœ… Testes end-to-end com arquivos reais
- âœ… Performance: <20ms por arquivo

### v3.0
- âœ… SeparaÃ§Ã£o de controle de concorrÃªncia em mÃ³dulos dedicados
- âœ… `scraper_manager/`: ConcorrÃªncia, proxies, circuit breaker, rate limiting
- âœ… `discovery_manager/`: Serper API, cache, Google fallback
- âœ… `concurrency_manager/`: OrquestraÃ§Ã£o global, resource pool, priority queue
- âœ… Circuit Breaker com estados (CLOSED/OPEN/HALF_OPEN) e recovery automÃ¡tico
- âœ… Proxy Pool com quarentena e teste de latÃªncia
- âœ… Cache de buscas com TTL e LRU eviction
- âœ… Preparado para +500 chamadas consecutivas sem gargalos
- âœ… SimplificaÃ§Ã£o: removidos endpoints/artefatos de teste e logs de diagnÃ³stico

### v2.2
- âœ… Batch Scraping: 3-5x mais rÃ¡pido que sequencial (delays variÃ¡veis 3-7s)
- âœ… Simula navegaÃ§Ã£o humana para evitar detecÃ§Ã£o de bot
- âœ… ConfigurÃ¡vel por ambiente (batch_size, delays)

### v2.1
- âœ… Sistema RESCUE para main pages com < 500 chars
- âœ… DocumentaÃ§Ã£o interativa completa
- âœ… Teste de stress com 300 empresas

### v2.0
- âœ… Scraper adaptativo com mÃºltiplas estratÃ©gias
- âœ… LLM Provider Manager com fallback
- âœ… Circuit Breaker por domÃ­nio
- âœ… Learning Engine

### v1.0
- âœ… Scraper bÃ¡sico com curl_cffi
- âœ… Discovery via Serper
- âœ… AnÃ¡lise LLM simples

## ğŸ“„ LicenÃ§a

ProprietÃ¡rio - Uso interno apenas.

---

*DocumentaÃ§Ã£o gerada em Dezembro 2025*
