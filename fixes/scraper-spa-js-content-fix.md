# Corre√ß√£o: Scraper n√£o extrai conte√∫do de sites SPA e JS-heavy

**Data:** 2025-12-03  
**Arquivo:** `app/services/scraper.py`  
**Casos:** DELTA SOLUCOES (deltaaut.com), DAVI MECANICA (davimecanicadiesel.com.br)

## Problema

### Caso 1: DELTA SOLUCOES (deltaaut.com)
- Site foi identificado corretamente
- Scraper retornou apenas 163 caracteres e 0 links
- Site √© uma SPA (Single Page Application) com todo conte√∫do em uma √∫nica p√°gina
- `PruningContentFilter` estava removendo conte√∫do v√°lido

### Caso 2: DAVI MECANICA (davimecanicadiesel.com.br)
- Site usa JavaScript pesado para renderizar conte√∫do
- Scraper retornava conte√∫do vazio porque JS n√£o carregava a tempo

## Causa Raiz

1. **PruningContentFilter muito agressivo:** 
   - `threshold=0.35` e `min_word_threshold=5` removiam conte√∫do v√°lido de sites com estrutura n√£o-convencional

2. **Falta de espera pelo JavaScript:** 
   - O crawler n√£o aguardava o carregamento completo do JS antes de extrair conte√∫do

3. **Sem fallback para SPAs:** 
   - Sites sem links internos (SPAs) n√£o eram tratados adequadamente

## Solu√ß√£o Implementada

### 1. Reduzir agressividade do PruningContentFilter

```python
# Antes:
md_generator = DefaultMarkdownGenerator(content_filter=PruningContentFilter(threshold=0.35, min_word_threshold=5))

# Depois:
md_generator = DefaultMarkdownGenerator(content_filter=PruningContentFilter(threshold=0.20, min_word_threshold=3))
```

### 2. Adicionar wait_for="networkidle"

```python
run_config = CrawlerRunConfig(
    cache_mode=CacheMode.BYPASS, 
    exclude_external_images=True, 
    markdown_generator=md_generator, 
    page_timeout=60000,
    wait_for="networkidle"  # NOVO: Aguardar rede ficar ociosa (JS carregado)
)
```

### 3. Retry autom√°tico para conte√∫do muito pequeno

```python
# Se conte√∫do muito pequeno, aguardar mais e tentar novamente
if result.success and result.markdown and len(result.markdown) < 500:
    logger.warning(f"‚ö†Ô∏è Conte√∫do muito pequeno ({len(result.markdown)} chars), aguardando JS e tentando novamente...")
    await asyncio.sleep(3)  # Aguardar JS renderizar
    result = await crawler.arun(url=url, config=run_config, magic=True)
```

### 4. Fallback para SPAs sem links

```python
# Se n√£o encontrou links mas o conte√∫do da main page √© substancial, usar apenas o conte√∫do principal
if len(links) == 0 and main_content_size > 500:
    logger.warning(f"‚ö†Ô∏è [SPA DETECTADO] Site sem links internos mas com conte√∫do substancial ({main_content_size} chars)")
    logger.info(f"üìù Usando apenas conte√∫do da p√°gina principal (poss√≠vel SPA ou site one-page)")
    return "\n".join(aggregated_markdown), list(all_pdf_links), visited_urls
```

## Logs Relacionados

Para identificar estes problemas no futuro, procurar nos logs:
- `[SPA DETECTADO]` - Indica que o fallback para SPA foi ativado
- `‚ö†Ô∏è Conte√∫do muito pequeno` - Indica retry autom√°tico por JS lento
- `links=0` com `markdown_chars` baixo - Indica poss√≠vel problema de extra√ß√£o

## Impacto

- Sites SPA como deltaaut.com agora ter√£o todo o conte√∫do extra√≠do corretamente
- Sites com JS pesado como davimecanicadiesel.com.br carregar√£o completamente antes da extra√ß√£o
- Melhor cobertura de conte√∫do em geral devido ao filtro menos agressivo

